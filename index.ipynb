{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-03T14:25:58.642346200Z",
     "start_time": "2025-04-03T14:25:58.514347700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pymongo\n",
    "from numpy import log10\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"solve\"]\n",
    "collection = db[\"data\"]\n",
    "\n",
    "\n",
    "class GWDataset(Dataset):\n",
    "    def __init__(self, data, x_scaler=None, y_scaler=None, fit_scalers=True):\n",
    "        self.data = data\n",
    "\n",
    "        params = np.array([[log10(item['r']), item['n_t'], log10(item['kappa10']),\n",
    "                            log10(item['T_re']), item['DN_re']] for item in data])\n",
    "        curves = np.array([np.column_stack((item['f_interp'],\n",
    "                                            item['log10OmegaGW_interp']))\n",
    "                           for item in data])\n",
    "\n",
    "        # 分割x和y\n",
    "        curves_x = curves[:, :, 0]\n",
    "        curves_y = curves[:, :, 1]\n",
    "\n",
    "        if fit_scalers or x_scaler is None:\n",
    "            self.param_scaler = StandardScaler()\n",
    "            self.param_scaler.fit(params)\n",
    "            self.x_scaler = StandardScaler()\n",
    "            self.x_scaler.fit(curves_x.reshape(-1, 1))\n",
    "            self.y_scaler = StandardScaler()\n",
    "            self.y_scaler.fit(curves_y.reshape(-1, 1))\n",
    "        else:\n",
    "            self.param_scaler = x_scaler\n",
    "            self.x_scaler = x_scaler\n",
    "            self.y_scaler = y_scaler\n",
    "\n",
    "        self.params = self.param_scaler.transform(params)\n",
    "        curves_x_scaled = self.x_scaler.transform(curves_x.reshape(-1, 1)).reshape(curves_x.shape)\n",
    "        curves_y_scaled = self.y_scaler.transform(curves_y.reshape(-1, 1)).reshape(curves_y.shape)\n",
    "        self.curves = np.stack([curves_x_scaled, curves_y_scaled], axis=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        params = torch.tensor(self.params[idx], dtype=torch.float32)\n",
    "        curve = torch.tensor(self.curves[idx], dtype=torch.float32)\n",
    "        return params, curve\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    params, curves = zip(*batch)\n",
    "    return torch.stack(params), torch.stack(curves)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class CurvePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 参数编码器\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(5, 128),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(256)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=256,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            bidirectional=False,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 编码参数 [B,5] -> [B,256]\n",
    "        encoded = self.encoder(x)\n",
    "\n",
    "        # 扩展为序列 [B,256] -> [B,256,256]\n",
    "        repeated = encoded.unsqueeze(1).repeat(1, 256, 1)\n",
    "\n",
    "        # 双向LSTM处理 [B,256,256] -> [B,256,512]\n",
    "        lstm_out, _ = self.lstm(repeated)\n",
    "\n",
    "        # 解码输出 [B,256,512] -> [B,256,2]\n",
    "        return self.decoder(lstm_out)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "train_losses = []  \n",
    "val_losses = []   \n",
    "\n",
    "def train_gw_model(condition={}, epochs=200, batch_size=32):\n",
    "    raw_data = list(collection.find(condition))\n",
    "    full_dataset = GWDataset(raw_data)\n",
    "    print(f'data num:{len(raw_data)}')\n",
    "\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        np.arange(len(full_dataset)),\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    train_data = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "    val_data = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CurvePredictor().to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=5\n",
    "    )\n",
    "    criterion = nn.MSELoss()\n",
    "    print('start training')\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for params, curves in train_loader:\n",
    "            params = params.to(device)\n",
    "            curves = curves.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(params)\n",
    "            loss = criterion(outputs, curves)\n",
    "            # loss_last = criterion(outputs[:,-1, :], curves[:,-1,:]) * 5.0  # 权重设为5\n",
    "            # loss_rest = criterion(outputs[:, :, :], curves[:, :, :])\n",
    "            # loss = loss_last + loss_rest\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * params.size(0)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for params, curves in val_loader:\n",
    "                params = params.to(device)\n",
    "                curves = curves.to(device)\n",
    "                outputs = model(params)\n",
    "                val_loss += criterion(outputs, curves).item() * params.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4e} | Val Loss: {val_loss:.4e}\")\n",
    "\n",
    "        # if val_loss < best_loss:\n",
    "        #     best_loss = val_loss\n",
    "        #     torch.save({\n",
    "        #         'model_state': model.state_dict(),\n",
    "        #         'x_scaler': full_dataset.x_scaler,\n",
    "        #         'y_scaler': full_dataset.y_scaler,\n",
    "        #         'param_scaler': full_dataset.param_scaler\n",
    "        #     }, 'best_gw_model.pth')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class GWPredictor:\n",
    "    def __init__(self, model_path='best_gw_model.pth'):\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "        self.model = CurvePredictor()\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        # self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.x_scaler = checkpoint['x_scaler']\n",
    "        self.y_scaler = checkpoint['y_scaler']\n",
    "        self.param_scaler = checkpoint['param_scaler']\n",
    "\n",
    "    def predict(self, params_dict):\n",
    "        params = np.array([\n",
    "            log10(params_dict['r']),\n",
    "            params_dict['n_t'],\n",
    "            log10(params_dict['kappa10']),\n",
    "            log10(params_dict['T_re']),\n",
    "            params_dict['DN_re']\n",
    "        ]).reshape(1, -1)\n",
    "\n",
    "        scaled_params = self.param_scaler.transform(params)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(scaled_params, dtype=torch.float32)\n",
    "            # inputs = inputs.to(self.device)\n",
    "            outputs = self.model(inputs).to('cpu').numpy()\n",
    "\n",
    "        # denorm = self.y_scaler.inverse_transform(\n",
    "        #     outputs.reshape(-1, 2)).reshape(outputs.shape)\n",
    "        denorm_x = self.x_scaler.inverse_transform(outputs[..., 0].reshape(-1, 1)).reshape(outputs.shape[0], -1)\n",
    "        denorm_y = self.y_scaler.inverse_transform(outputs[..., 1].reshape(-1, 1)).reshape(outputs.shape[0], -1)\n",
    "\n",
    "        return {\n",
    "            'f': denorm_x[0].tolist(),\n",
    "            'log10OmegaGW': denorm_y[0].tolist()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca9037f9ef85724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T05:48:39.412376700Z",
     "start_time": "2025-04-03T05:47:53.281881200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data num:25689\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:27<44:55, 27.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 3.1100e-01 | Val Loss: 1.1336e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:31<52:11, 31.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [21]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m trained_model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_gw_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# 'r': {'$gte': 1e-6, '$lte': 1e-4},\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# 'n_t': {'$gte': 0, '$lte': 1},\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# 'kappa10': {'$gte': 1e2, '$lte': 2e2},\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# 'T_re': {'$gte': 0, '$lte': 1e3},\u001B[39;49;00m\n\u001B[0;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# 'DN_re': {'$gte': 20, '$lte': 40}\u001B[39;49;00m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36mtrain_gw_model\u001B[1;34m(condition, epochs, batch_size)\u001B[0m\n\u001B[0;32m    166\u001B[0m curves \u001B[38;5;241m=\u001B[39m curves\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    168\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 169\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, curves)\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# loss_last = criterion(outputs[:,-1, :], curves[:,-1,:]) * 5.0  # 权重设为5\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# loss_rest = criterion(outputs[:, :, :], curves[:, :, :])\u001B[39;00m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;66;03m# loss = loss_last + loss_rest\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\py\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36mCurvePredictor.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    106\u001B[0m repeated \u001B[38;5;241m=\u001B[39m encoded\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mrepeat(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    108\u001B[0m \u001B[38;5;66;03m# 双向LSTM处理 [B,256,256] -> [B,256,512]\u001B[39;00m\n\u001B[1;32m--> 109\u001B[0m lstm_out, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepeated\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;66;03m# 解码输出 [B,256,512] -> [B,256,2]\u001B[39;00m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(lstm_out)\n",
      "File \u001B[1;32m~\\.conda\\envs\\py\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\py\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:911\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    908\u001B[0m         hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[0;32m    910\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 911\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    912\u001B[0m \u001B[43m                      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_first\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    913\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    914\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, batch_sizes, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias,\n\u001B[0;32m    915\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = train_gw_model(\n",
    "    {\n",
    "        # 'r': {'$gte': 1e-6, '$lte': 1e-4},\n",
    "        # 'n_t': {'$gte': 0, '$lte': 1},\n",
    "        # 'kappa10': {'$gte': 1e2, '$lte': 2e2},\n",
    "        # 'T_re': {'$gte': 0, '$lte': 1e3},\n",
    "        # 'DN_re': {'$gte': 20, '$lte': 40}\n",
    "    }, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.save('trainloss',train_losses)\n",
    "np.save('valloss',val_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-03T05:48:39.402971900Z"
    }
   },
   "id": "5d5c37d9d27a216e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 100\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), sharex=False)\n",
    "\n",
    "ax1.plot(range(1, epochs + 1), train_losses, 'g--', label='Train Loss', marker='*', color=\"royalblue\")\n",
    "ax1.plot(range(1, epochs + 1), val_losses, 'b--', label='Validation Loss', marker='.', color=\"red\")\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss Curves')\n",
    "ax1.legend()\n",
    "ax1.grid(False)\n",
    "\n",
    "ax2.plot(range(51, epochs + 1), train_losses[-50:], 'g--', label='Train Loss', marker='*', color=\"royalblue\")\n",
    "ax2.plot(range(51, epochs + 1), val_losses[-50:], 'b--', label='Validation Loss', marker='.', color=\"red\")\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Training and Validation Loss Curves after 50 Epochs')\n",
    "ax2.legend()\n",
    "ax2.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('./image/combined_train_loss.eps', format='eps', dpi=50, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-03T05:48:39.406971300Z"
    }
   },
   "id": "ac999e73721e8eb3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
